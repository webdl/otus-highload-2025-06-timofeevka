# Масштабируемая подсистема диалогов

## Оглавление

- [Подготовка](#подготовка)
    - [Разработка архитектуры сервиса диалогов](#разработка-архитектуры-сервиса-диалогов)
    - [Переосмысление авторизации](#переосмысление-авторизации)
- [Будущие улучшения](#будущие-улучшения)
- [Реализация сервиса диалогов](#реализация-сервиса-диалогов)
    - [Критический путь](#критический-путь)
    - [Шардирование сообщений](#шардирование-сообщений)
    - [Проверка шардирования](#проверка-шардирования)
- [Добавление нового шарда](#добавление-нового-шарда)
- [Удаление шарда](#удаление-шарда)

## Подготовка

### Разработка архитектуры сервиса диалогов

С целью систематизации разрабатываемых сервисов было принято решение сделать шаг назад и проработать архитектуру всего продукта.
Всё началось с выделения бизнес-контекста, целей, стейкхолдеров и составления первичных бизнес-требований. Подробнее про это вы можете
почитать в разделе документации [arch](/doc/arch). Этот документ ложится в основу как ранее созданных сервисов, так и нового сервиса
диалогов.

Вторым этапом я описал по модели C4 первые 3 уровня всех микросервисов (за исключением user-service). Это позволило спроектировать
взаимодействие компонентов сервиса, а дальше оставалось дело техники — написание самого кода.

Схема для сервиса диалогов выглядит следующим образом:

![C3-chats-service.svg](/doc/arch/c4/C3-chats-service.svg)

Данная схема достаточно подробно описывает связи компонентов, но всё еще скрывает детали реализации, описываемые на 4-м уровне модели C4,
что мне и требовалось. Так, например, если сравнивать ее с кодом [chat-service](/chat-service), вы непременно найдете компонены с
одинаковыми именами и связи между ними будут выстроены согласно схеме. Но это будет лишь часть классов, которые потребовались для
создания сервиса диалогов.

### Переосмысление авторизации

В предыдущих версиях продукта я делал публичное REST API, в которое можно обращаться без авторизации. Предполагалось, что сервис
аутентификации будет пропускать только авторизованные запросы к таким сервисам. Этот подход претерпел важное изменение: вместо публичного
API, или необходимости передавать в каждом теле запроса идентификатор пользователя, я поместил его в заголовки запроса (headers). Таким
образом задача сервиса аутентификации "обоготить" запрос от пользователя дополнительными заголовками и направить его дальше.

С точки зрения кода такой подход прост и лаконичен, ведь разработчику достаточно использовать аннотацию чтения заголовком чтобы
гарантировать авторизованный доступ к своему сервису.

## Будущие улучшения

На проектирование и реализацию сервиса ушло немало времени. Была проделана большая работа по изучению материалов по построению системы
диалогов с точки зрения архитектуры и реализации, поэтому не всё задуманное удалось реализовать. Список ниже содержит перечень улучшений
функциональности, надежности и производительности, которые не влияют на домашнее задание, но могли бы быть сделаны в будущем:

1. Мониторинг MongoDB в Grafana
2. Механизм применеия схемы данных из кода в MongoDB
3. Мета-информация для каждого пользователя по их чатам (непрочитанные сообщения и т.п.)
4. Кэширование: сообщений в чатах, мета-информации о чатах
5. Пагинация сообщений в чатах
6. Групповые чаты
7. Улучшенный механизм борьбы с "эффектом Леди Гаги", включая пагинацию и сортировку сообщений
8. Добавить авторизацию в MongoDB
9. Пересылка сообщений между чатами
10. Пакетная обработка сообщений (операции удаления и пересылки)

## Реализация сервиса диалогов

Сервис диалогов состоит из двух ключевых API:

Управление чатами ([ChatController.java](/chat-service/src/main/java/ru/webdl/otus/socialnetwork/infra/chat/ChatController.java)):

```
POST     /chats           # Создать новый чат
GET      /chats           # Получить список чатов пользователя
DELETE   /chats/{chatId}  # Удалить чат
```

Управление
сообщениями ([MessageController.java](/chat-service/src/main/java/ru/webdl/otus/socialnetwork/infra/message/MessageController.java)):

```
POST     /chats/{chatId}/messages          # Отправить сообщение
GET      /chats/{chatId}/messages          # Прочитать все сообщения
PUT      /chats/{chatId}/messages/{msgId}  # Изменить сообщение
DELETE   /chats/{chatId}/messages/{msgId}  # Удалить сообщение
```

В файле [postman_collection.json](/user-service/src/test/postman/postman_collection.json) перечислены все доступные вызовы API.

Запросы через REST-контроллеры идут из слоя инфраструктуры в доменный слой, где происходит обработка бизнес-логики — это реализация
[Чистой архитектуры](https://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html). Преимущества этого подхода хорошо
прослеживается при сравнении файлов из разных слоев:

- Домен: [MessageImpl.java](/chat-service/src/main/java/ru/webdl/otus/socialnetwork/core/message/MessageImpl.java)
- Инфра: [MongoMessage.java](/chat-service/src/main/java/ru/webdl/otus/socialnetwork/infra/message/MongoMessage.java)

где объект "сообщение" из бизнес-слоя содержит только необходимые для бизнес-логики поля, а его проекция в слое инфраструктуры знает
детали реализации шардирования и хранения данных в MongoDB.

### Критический путь

При начале диалога между двумя пользователями фронтенд посылает запрос на создание нового чата в
[MessageCreationUseCase.java](/chat-service/src/main/java/ru/webdl/otus/socialnetwork/core/message/MessageCreationUseCase.java). На этом
этапе может выполняться проверка прав на чат, Event Driven уведомления и другая бизнес-логика.

Сообщения в чат отправляются
через [MessageCreationUseCase.java](/chat-service/src/main/java/ru/webdl/otus/socialnetwork/core/message/MessageCreationUseCase.java),
где также может выполняться дополнительная бизнес-логика, а сейчас на этом этапе выполняется главное:

1. Определяется ключ шардирования для снижения "эффекта Леди Гаги" (об этом далее)
2. В мета-информации чата сохраняется последнее сообщение для его сортировки и отображения в списке чатов собеседника
3. Сообщение сохраняется в БД

### Шардирование сообщений

В самом простом виде шардировать сообщения в MongoDB можно по атрибуту `chatId` — это удобно потому, что на каждом шарде будет храниться
вся история одного чата. Обратной стороной этого подхода является "эффект Леди Гаги", когда чаты с большим количеством сообщений хранятся
на одном шарде, тем самым нагружая его сильнее других.

Для решения этой проблемы применяется различные подходы:

1. **Buckets:** сообщения записываются в так называется бакеты, которые вмещают в себя ограниченное количество сообщений (например: не
   более 10,000). Ключем шардирования выступает UUID бакета. Альтернативой может выступать реализация, когда сообщения по структуре данных
   не находятся в бакетах, но имеет его ID в теле.
2. **Time range:** сообщения бьются на временные диапазоны: неделя, месяц и т.д. Ключем шардирования выступает диапазон.
3. **Гибридный:** комбинации подходов выше в зависимости от решаемых задач (пагинация, быстрый доступ к последним сообщениям и т.д.)

**Интересно знать,** что в крупных системах, по типу Telegram, используется сложная гибридная архитектуру хранения сообщений, которая
разработана под высокую производительность и оптимальную стоимость:

```
1. RAM Cache → Горячие диалоги и активные чаты
2. SSD Storage → Недавние сообщения (последние 1-3 месяца)
3. HDD Archive → Исторические данные (старые сообщения)
4. Cold Storage → Очень старые данные (редко запрашиваемые)
```

Поэтому решение подобной задачи требует тщательной проработки.

В моем случае я остановился на комбинации `chat UUID + Time range`, где `Time range` выполнен в формате `yyMMddHH` от даты создания
сообщения, то есть меняется каждый час. Данный shard key вычисляется для каждого нового сообщения и помещается в переменную `compositeHash`.
Пример: `4b9f2cca-5bde-41cc-96e8-8891f4e58036|25101404`.

Код реализации находится в
[MongoMessageEventListener.java](/chat-service/src/main/java/ru/webdl/otus/socialnetwork/infra/message/MongoMessageEventListener.java).

Чтобы шардирование заработало в MongoDB необходимо подключиться к нашему серверу через консоль (напр. в MongoDB Compass) и выполнить
команды:

```shell
use chatdb
sh.enableSharding("chatdb")
db.messages.createIndex({"compositeHash": "hashed"})
sh.shardCollection("chatdb.messages", {"compositeHash": "hashed"})
```

Это создаст hash-индекс для данного поля и переведет БД в режим шардирования.

### Проверка шардирования

Для домашнего задания я временно поменял способ генерации `bucketId` — теперь он содержит значения в формате `MMddHHmm`, то есть
месяц-день-час-минута. Это позволило быстрее сгенерировать разные значения атрибута `compositeHash`. Это важный момент, так как для
шардирования у Shared key должна быть достаточная кардинальность, а если у вас мало сообщений с уникальным Shared key, то Mongo не будет
шардировать такие данные. Генерацию происходил путем вызова REST API через Postman.

После того, как было сгенерировано 1400 объектов можно посмотреть, как они распределились:

```shell
db.messages.getShardDistribution()
```

```mongodb-json
Shard shard2 at shard2/mongoshard21:27017,mongoshard22:27017,mongoshard23:27017
{
  data: '474KiB',
  docs: 1397,
  chunks: 1,
  'estimated data per chunk': '474KiB',
  'estimated docs per chunk': 1397
}

Totals
{
  data: '474KiB',
  docs: 1397,
  chunks: 1,
  'Shard shard2': [
    '100 % data',
    '100 % docs in cluster',
    '348B avg obj size on shard'
  ]
}
```

Видно, что они всё еще находятся на одном шарде. Вот почему это происходит: Mongo шардирует не сами объекты, а чанки (в одном чанке
множество объектов). Стандартный размер одного чанка 64MiB, а у меня данных только на 474KiB.

Получается, что пока данных не будет достаточно, вся затея с разным `compositeHash` — впустую? Не совсем так.

Каждый чанк может хранить только те объекты, которые удовлетворяют условиям шардирования, посмотреть которые можно по команде:

```
sh.status()
```

При начале жизни коллекции там можно увидеть запись вида:

```mongodb-json
...
chunks: [
          {
            min: {
              compositeHash: MinKey()
            },
            max: {
              compositeHash: MaxKey()
            },
            'on shard': 'shard2',
            'last modified': Timestamp({ t: 1, i: 0 })
          }
      ]
...
```

Которая означает, что в данный чанк попадают все возможные значения вычисления хэша от compositeHash.

Ситуация меняется с увеличением количества данных. Балансировщик в Mongo, зная, что коллекция шардирована и в распоряжении у него 2 шарда,
оптимизирует хранение и создает дополнительные чанки:

```mongodb-json
...
chunks: [
          {
            min: {
              compositeHash: MinKey()
            },
            max: {
              compositeHash: -9094555246894692000
            },
            'on shard': 'shard2',
            'last modified': Timestamp({ t: 1, i: 0 })
          },
          {
            min: {
              compositeHash: -9094555246894692000
            },
            max: {
              compositeHash: -8996886102120246000
            },
            'on shard': 'shard1',
            'last modified': Timestamp({ t: 1, i: 1 })
          },
...
```

Таким образом он распределяет данные уже по многим чанкам, а их уже можно шардировать:

```shell
db.messages.getShardDistribution()
```

```mongodb-json
Shard shard1 at shard1/mongoshard11:27017,mongoshard12:27017,mongoshard13:27017
{
  data: '1012KiB',
  docs: 2980,
  chunks: 45,
  'estimated data per chunk': '22KiB',
  'estimated docs per chunk': 66
}

Shard shard2 at shard2/mongoshard21:27017,mongoshard22:27017,mongoshard23:27017
{
  data: '824KiB',
  docs: 2427,
  chunks: 45,
  'estimated data per chunk': '18KiB',
  'estimated docs per chunk': 53
}

Totals
{
  data: '1.79MiB',
  docs: 5407,
  chunks: 90,
  'Shard shard1': [
    '55.11 % data',
    '55.11 % docs in cluster',
    '348B avg obj size on shard'
  ],
  'Shard shard2': [
    '44.88 % data',
    '44.88 % docs in cluster',
    '348B avg obj size on shard'
  ]
}
```

**А где тут Леди Гага?**

Стоит сказать, что все 5.4k сообщений записаны в один чат. А раз они распределились по двум шардам то мы с уверенностью можем сказать, 
что "эффект Леди Гаги" побежден.

## Добавление нового шарда

Проверить, как наши данные распределятся по трем шардам. Для этого в [docker-compose-mongodb.yml](/docker-compose-mongodb.yml) добавим
3-й шард и запустим его:

```yaml
...
  # Third Shard Cluster
  mongoshard31:
    build:
      context: deploy/mongo/build/replicaset
    container_name: mongoshard31
    depends_on:
      - mongoshard32
      - mongoshard33
    command: mongod --shardsvr --replSet shard3 --dbpath /data/db --port 27017
    environment:
      - REPLICA_SET=shard3
    expose:
      - "27017"
    volumes:
      - shard_d31:/data/db
    networks:
      - socialnet
  mongoshard32:
    image: mongo
    container_name: mongoshard32
    command: mongod --shardsvr --replSet shard3 --dbpath /data/db --port 27017
    expose:
      - "27017"
    volumes:
      - shard_d32:/data/db
    networks:
      - socialnet
  mongoshard33:
    image: mongo
    container_name: mongoshard33
    command: mongod --shardsvr --replSet shard3 --dbpath /data/db --port 27017
    expose:
      - "27017"
    volumes:
      - shard_d33:/data/db
    networks:
      - socialnet
...
volumes:
  ...
  shard_d31:
  shard_d32:
  shard_d33:
...
```

Далее в самом Mongo нужно добавить новый шард:

```
sh.addShard("shard3/mongoshard31:27017,mongoshard32:27017,mongoshard33:27017")
```

```mongodb-json
{
  shardAdded: 'shard3',
  ok: 1,
  '$clusterTime': {
    clusterTime: Timestamp({ t: 1760525176, i: 18 }),
    signature: {
      hash: Binary.createFromBase64('AAAAAAAAAAAAAAAAAAAAAAAAAAA=', 0),
      keyId: Long('0')
    }
  },
  operationTime: Timestamp({ t: 1760525176, i: 18 })
}
```

После этого даем команду Mongo на решардинг данных:

```mongodb-json
db.adminCommand({
    reshardCollection: "chatdb.messages",
    key: {"compositeHash": "hashed"},
    forceRedistribution: true
})
```

И смотрим за процессом через команду:

```
sh.getShardedDataDistribution()
```

```mongodb-json
{
  ns: 'chatdb.system.resharding.f81153bd-2f07-4074-9aab-99d29af4c873',
  shards: [
    {
      shardName: 'shard1',
      numOrphanedDocs: 0,
      numOwnedDocuments: 1528,
      ownedSizeBytes: 531744,
      orphanedSizeBytes: 0
    },
    {
      shardName: 'shard3',
      numOrphanedDocs: 0,
      numOwnedDocuments: 2019,
      ownedSizeBytes: 702612,
      orphanedSizeBytes: 0
    },
    {
      shardName: 'shard2',
      numOrphanedDocs: 0,
      numOwnedDocuments: 1860,
      ownedSizeBytes: 647280,
      orphanedSizeBytes: 0
    }
  ]
}
```

**Обратите внимание!** Команда выполняется ~10 мин. даже для такого небольшого количества объектов.

Смотрим результат:

```
db.messages.getShardDistribution()
```

```mongodb-json
Shard shard3 at shard3/mongoshard31:27017,mongoshard32:27017,mongoshard33:27017
{
  data: '686KiB',
  docs: 2019,
  chunks: 30,
  'estimated data per chunk': '22KiB',
  'estimated docs per chunk': 67
}

Shard shard2 at shard2/mongoshard21:27017,mongoshard22:27017,mongoshard23:27017
{
  data: '632KiB',
  docs: 1860,
  chunks: 30,
  'estimated data per chunk': '21KiB',
  'estimated docs per chunk': 62
}

Shard shard1 at shard1/mongoshard11:27017,mongoshard12:27017,mongoshard13:27017
{
  data: '519KiB',
  docs: 1528,
  chunks: 30,
  'estimated data per chunk': '17KiB',
  'estimated docs per chunk': 50
}

Totals
{
  data: '1.79MiB',
  docs: 5407,
  chunks: 90,
  'Shard shard3': [
    '37.34 % data',
    '37.34 % docs in cluster',
    '348B avg obj size on shard'
  ],
  'Shard shard2': [
    '34.39 % data',
    '34.39 % docs in cluster',
    '348B avg obj size on shard'
  ],
  'Shard shard1': [
    '28.25 % data',
    '28.25 % docs in cluster',
    '348B avg obj size on shard'
  ]
}
```

## Удаление шарда

Выполняем удаление в обратном порядке. Удаляем шард:

```
db.adminCommand( {removeShard: ("shard3")} )
```

```mongodb-json

  msg: 'draining started successfully',
  state: 'started',
  shard: 'shard3',
  dbsToMove: [],
  collectionsToMove: [],
  ok: 1,
  '$clusterTime': {
    clusterTime: Timestamp({ t: 1760526207, i: 3 }),
    signature: {
      hash: Binary.createFromBase64('AAAAAAAAAAAAAAAAAAAAAAAAAAA=', 0),
      keyId: Long('0')
    }
  },
  operationTime: Timestamp({ t: 1760526207, i: 3 })
}
```

Периодически выполняя команду ниже смотрим, как документы перераспределяются между шардами:

```
sh.getShardedDataDistribution()
```

```mongodb-json
{
  ns: 'chatdb.messages',
  shards: [
    {
      shardName: 'shard1',
      numOrphanedDocs: 0,
      numOwnedDocuments: 2120,
      ownedSizeBytes: 737760,
      orphanedSizeBytes: 0
    },
    {
      shardName: 'shard2',
      numOrphanedDocs: 0,
      numOwnedDocuments: 2131,
      ownedSizeBytes: 741588,
      orphanedSizeBytes: 0
    },
    {
      shardName: 'shard3',
      numOrphanedDocs: 863,
      numOwnedDocuments: 1156,
      ownedSizeBytes: 402288,
      orphanedSizeBytes: 300324
    }
  ]
}
```

```mongodb-json
{
  ns: 'chatdb.messages',
  shards: [
    {
      shardName: 'shard1',
      numOrphanedDocs: 0,
      numOwnedDocuments: 2460,
      ownedSizeBytes: 856080,
      orphanedSizeBytes: 0
    },
    {
      shardName: 'shard3',
      numOrphanedDocs: 1528,
      numOwnedDocuments: 491,
      ownedSizeBytes: 170868,
      orphanedSizeBytes: 531744
    },
    {
      shardName: 'shard2',
      numOrphanedDocs: 0,
      numOwnedDocuments: 2456,
      ownedSizeBytes: 854688,
      orphanedSizeBytes: 0
    }
  ]
}
```

И вот наши чанки снова распределены по двум шардам:

```mongodb-json
{
  data: '1.79MiB',
  docs: 5407,
  chunks: 90,
  'Shard shard1': [
    '50.98 % data',
    '50.98 % docs in cluster',
    '348B avg obj size on shard'
  ],
  'Shard shard2': [
    '49.01 % data',
    '49.01 % docs in cluster',
    '348B avg obj size on shard'
  ]
}
```

После этого можно отключить сервера третьего шарда.
